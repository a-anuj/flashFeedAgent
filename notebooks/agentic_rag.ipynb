{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a85d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install google-generativeai\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a41eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d766910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b884aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 21:23:23.819543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-04 21:23:24.536329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-04 21:23:27.174221: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List,TypedDict\n",
    "from langgraph.graph import StateGraph,END\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5237f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "emb = HuggingFaceBgeEmbeddings(model_name=\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d7125e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"https://docs.langchain.com/oss/python/langchain/\"\n",
    "names = [\"overview\",\"install\",\"quickstart\",\"philosophy\",\"agents\",\"models\",\"messages\",\"tools\",\"short-term-memory\",\"streaming\",\"structured-output\",\"\"]\n",
    "urls = [format+name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d2d41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://docs.langchain.com/oss/python/langchain/overview',\n",
       " 'https://docs.langchain.com/oss/python/langchain/install',\n",
       " 'https://docs.langchain.com/oss/python/langchain/quickstart',\n",
       " 'https://docs.langchain.com/oss/python/langchain/philosophy',\n",
       " 'https://docs.langchain.com/oss/python/langchain/agents',\n",
       " 'https://docs.langchain.com/oss/python/langchain/models',\n",
       " 'https://docs.langchain.com/oss/python/langchain/messages',\n",
       " 'https://docs.langchain.com/oss/python/langchain/tools',\n",
       " 'https://docs.langchain.com/oss/python/langchain/short-term-memory',\n",
       " 'https://docs.langchain.com/oss/python/langchain/streaming',\n",
       " 'https://docs.langchain.com/oss/python/langchain/structured-output',\n",
       " 'https://docs.langchain.com/oss/python/langchain/']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LangChain overview - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.x is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content and API reference.\n",
      "LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\n",
      "We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\n",
      "LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\n",
      "â€‹ Create an agent\n",
      "Copy# pip install -qU langchain \"langchain[anthropic]\"\n",
      "from langchain.agents import create_agent\n",
      "\n",
      "def get_weather(city: str) -> str:\n",
      "    \"\"\"Get weather for a given city.\"\"\"\n",
      "    return f\"It's always sunny in {city}!\"\n",
      "\n",
      "agent = create_agent(\n",
      "    model=\"claude-sonnet-4-5-20250929\",\n",
      "    tools=[get_weather],\n",
      "    system_prompt=\"You are a helpful assistant\",\n",
      ")\n",
      "\n",
      "# Run the agent\n",
      "agent.invoke(\n",
      "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
      ")\n",
      "\n",
      "See the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\n",
      "â€‹ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChainâ€™s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChainâ€™s agents are built on top of LangGraph. This allows us to take advantage of LangGraphâ€™s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify' metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(web_paths=urls)\n",
    "document = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ee40a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Messages - Docs by LangChainSkip to main contentðŸš€ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsMessagesLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageBasic usageText promptsMessage promptsDictionary formatMessage typesSystem MessageHuman MessageText contentMessage metadataAI MessageTool callsToken usageStreaming and chunksTool MessageMessage contentStandard content blocksMultimodalContent block referenceUse with chat modelsCore componentsMessagesCopy pageCopy pageMessages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.\n",
      "Messages are objects that contain:\n",
      "\n",
      " Role - Identifies the message type (e.g. system, user)\n",
      " Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n",
      " Metadata - Optional fields such as response information, message IDs, and token usage\n",
      "\n",
      "LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.\n",
      "â€‹Basic usage\n",
      "The simplest way to use messages is to create message objects and pass them to a model when invoking.\n",
      "Copyfrom langchain.chat_models import init_chat_model\n",
      "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "system_msg = SystemMessage(\"You are a helpful assistant.\")\n",
      "human_msg = HumanMessage(\"Hello, how are you?\")\n",
      "\n",
      "# Use with chat models\n",
      "messages = [system_msg, human_msg]\n",
      "response = model.invoke(messages)  # Returns AIMessage\n",
      "\n",
      "â€‹Text prompts\n",
      "Text prompts are strings - ideal for straightforward generation tasks where you donâ€™t need to retain conversation history.\n",
      "Copyresponse = model.invoke(\"Write a haiku about spring\")\n",
      "\n",
      "Use text prompts when:\n",
      "\n",
      "You have a single, standalone request\n",
      "You donâ€™t need conversation history\n",
      "You want minimal code complexity\n",
      "\n",
      "â€‹Message prompts\n",
      "Alternatively, you can pass in a list of messages to the model by providing a list of message objects.\n",
      "Copyfrom langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
      "\n",
      "messages = [\n",
      "    SystemMessage(\"You are a poetry expert\"),\n",
      "    HumanMessage(\"Write a haiku about spring\"),\n",
      "    AIMessage(\"Cherry blossoms bloom...\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "Use message prompts when:\n",
      "\n",
      "Managing multi-turn conversations\n",
      "Working with multimodal content (images, audio, files)\n",
      "Including system instructions\n",
      "\n",
      "â€‹Dictionary format\n",
      "You can also specify messages directly in OpenAI chat completions format.\n",
      "Copymessages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n",
      "    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "â€‹Message types\n",
      "\n",
      " System message - Tells the model how to behave and provide context for interactions\n",
      " Human message - Represents user input and interactions with the model\n",
      " AI message - Responses generated by the model, including text content, tool calls, and metadata\n",
      " Tool message - Represents the outputs of tool calls\n",
      "\n",
      "â€‹System Message\n",
      "A SystemMessage represent an initial set of instructions that primes the modelâ€™s behavior. You can use a system message to set the tone, define the modelâ€™s role, and establish guidelines for responses.\n",
      "Basic instructionsCopysystem_msg = SystemMessage(\"You are a helpful coding assistant.\")\n",
      "\n",
      "messages = [\n",
      "    system_msg,\n",
      "    HumanMessage(\"How do I create a REST API?\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "Detailed personaCopyfrom langchain.messages import SystemMessage, HumanMessage\n",
      "\n",
      "system_msg = SystemMessage(\"\"\"\n",
      "You are a senior Python developer with expertise in web frameworks.\n",
      "Always provide code examples and explain your reasoning.\n",
      "Be concise but thorough in your explanations.\n",
      "\"\"\")\n",
      "\n",
      "messages = [\n",
      "    system_msg,\n",
      "    HumanMessage(\"How do I create a REST API?\")\n",
      "]\n",
      "response = model.invoke(messages)\n",
      "\n",
      "\n",
      "â€‹Human Message\n",
      "A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.\n",
      "â€‹Text content\n",
      "Message objectString shortcutCopyresponse = model.invoke([\n",
      "  HumanMessage(\"What is machine learning?\")\n",
      "])\n",
      "\n",
      "â€‹Message metadata\n",
      "Add metadataCopyhuman_msg = HumanMessage(\n",
      "    content=\"Hello!\",\n",
      "    name=\"alice\",  # Optional: identify different users\n",
      "    id=\"msg_123\",  # Optional: unique identifier for tracing\n",
      ")\n",
      "\n",
      "The name field behavior varies by provider â€“ some use it for user identification, others ignore it. To check, refer to the model providerâ€™s reference.\n",
      "\n",
      "â€‹AI Message\n",
      "An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n",
      "Copyresponse = model.invoke(\"Explain AI\")\n",
      "print(type(response))  # <class 'langchain.messages.AIMessage'>\n",
      "\n",
      "AIMessage objects are returned by the model when calling it, which contains all of the associated metadata in the response.\n",
      "Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new AIMessage object and insert it into the message history as if it came from the model.\n",
      "Copyfrom langchain.messages import AIMessage, SystemMessage, HumanMessage\n",
      "\n",
      "# Create an AI message manually (e.g., for conversation history)\n",
      "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
      "\n",
      "# Add to conversation history\n",
      "messages = [\n",
      "    SystemMessage(\"You are a helpful assistant\"),\n",
      "    HumanMessage(\"Can you help me?\"),\n",
      "    ai_msg,  # Insert as if it came from the model\n",
      "    HumanMessage(\"Great! What's 2+2?\")\n",
      "]\n",
      "\n",
      "response = model.invoke(messages)\n",
      "\n",
      "Attributesâ€‹textstringThe text content of the message.â€‹contentstring | dict[]The raw content of the message.â€‹content_blocksContentBlock[]The standardized content blocks of the message.â€‹tool_callsdict[] | NoneThe tool calls made by the model.Empty if no tools are called.â€‹idstringA unique identifier for the message (either automatically generated by LangChain or returned in the provider response)â€‹usage_metadatadict | NoneThe usage metadata of the message, which can contain token counts when available.â€‹response_metadataResponseMetadata | NoneThe response metadata of the message.\n",
      "â€‹Tool calls\n",
      "When models make tool calls, theyâ€™re included in the AIMessage:\n",
      "Copyfrom langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "def get_weather(location: str) -> str:\n",
      "    \"\"\"Get the weather at a location.\"\"\"\n",
      "    ...\n",
      "\n",
      "model_with_tools = model.bind_tools([get_weather])\n",
      "response = model_with_tools.invoke(\"What's the weather in Paris?\")\n",
      "\n",
      "for tool_call in response.tool_calls:\n",
      "    print(f\"Tool: {tool_call['name']}\")\n",
      "    print(f\"Args: {tool_call['args']}\")\n",
      "    print(f\"ID: {tool_call['id']}\")\n",
      "\n",
      "Other structured data, such as reasoning or citations, can also appear in message content.\n",
      "â€‹Token usage\n",
      "An AIMessage can hold token counts and other usage metadata in its usage_metadata field:\n",
      "Copyfrom langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\")\n",
      "\n",
      "response = model.invoke(\"Hello!\")\n",
      "response.usage_metadata\n",
      "\n",
      "Copy{'input_tokens': 8,\n",
      " 'output_tokens': 304,\n",
      " 'total_tokens': 312,\n",
      " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
      " 'output_token_details': {'audio': 0, 'reasoning': 256}}\n",
      "\n",
      "See UsageMetadata for details.\n",
      "â€‹Streaming and chunks\n",
      "During streaming, youâ€™ll receive AIMessageChunk objects that can be combined into a full message object:\n",
      "Copychunks = []\n",
      "full_message = None\n",
      "for chunk in model.stream(\"Hi\"):\n",
      "    chunks.append(chunk)\n",
      "    print(chunk.text)\n",
      "    full_message = chunk if full_message is None else full_message + chunk\n",
      "\n",
      "Learn more:\n",
      "Streaming tokens from chat models\n",
      "Streaming tokens and/or steps from agents\n",
      "\n",
      "\n",
      "â€‹Tool Message\n",
      "For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.\n",
      "Tools can generate ToolMessage objects directly. Below, we show a simple example. Read more in the tools guide.\n",
      "Copyfrom langchain.messages import AIMessage\n",
      "from langchain.messages import ToolMessage\n",
      "\n",
      "# After a model makes a tool call\n",
      "# (Here, we demonstrate manually creating the messages for brevity)\n",
      "ai_message = AIMessage(\n",
      "    content=[],\n",
      "    tool_calls=[{\n",
      "        \"name\": \"get_weather\",\n",
      "        \"args\": {\"location\": \"San Francisco\"},\n",
      "        \"id\": \"call_123\"\n",
      "    }]\n",
      ")\n",
      "\n",
      "# Execute tool and create result message\n",
      "weather_result = \"Sunny, 72Â°F\"\n",
      "tool_message = ToolMessage(\n",
      "    content=weather_result,\n",
      "    tool_call_id=\"call_123\"  # Must match the call ID\n",
      ")\n",
      "\n",
      "# Continue conversation\n",
      "messages = [\n",
      "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
      "    ai_message,  # Model's tool call\n",
      "    tool_message,  # Tool execution result\n",
      "]\n",
      "response = model.invoke(messages)  # Model processes the result\n",
      "\n",
      "Attributesâ€‹contentstringrequiredThe stringified output of the tool call.â€‹tool_call_idstringrequiredThe ID of the tool call that this message is responding to. Must match the ID of the tool call in the AIMessage.â€‹namestringrequiredThe name of the tool that was called.â€‹artifactdictAdditional data not sent to the model but can be accessed programmatically.\n",
      "The artifact field stores supplementary data that wonâ€™t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the modelâ€™s context.Example: Using artifact for retrieval metadataFor example, a retrieval tool could retrieve a passage from a document for reference by a model. Where message content contains text that the model will reference, an artifact can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:Copyfrom langchain.messages import ToolMessage\n",
      "\n",
      "# Sent to model\n",
      "message_content = \"It was the best of times, it was the worst of times.\"\n",
      "\n",
      "# Artifact available downstream\n",
      "artifact = {\"document_id\": \"doc_123\", \"page\": 0}\n",
      "\n",
      "tool_message = ToolMessage(\n",
      "    content=message_content,\n",
      "    tool_call_id=\"call_123\",\n",
      "    name=\"search_books\",\n",
      "    artifact=artifact,\n",
      ")\n",
      "See the RAG tutorial for an end-to-end example of building retrieval agents with LangChain.\n",
      "\n",
      "â€‹Message content\n",
      "You can think of a messageâ€™s content as the payload of data that gets sent to the model. Messages have a content attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.\n",
      "Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\n",
      "LangChain chat models accept message content in the content attribute.\n",
      "This may contain either:\n",
      "\n",
      "A string\n",
      "A list of content blocks in a provider-native format\n",
      "A list of LangChainâ€™s standard content blocks\n",
      "\n",
      "See below for an example using multimodal inputs:\n",
      "Copyfrom langchain.messages import HumanMessage\n",
      "\n",
      "# String content\n",
      "human_message = HumanMessage(\"Hello, how are you?\")\n",
      "\n",
      "# Provider-native format (e.g., OpenAI)\n",
      "human_message = HumanMessage(content=[\n",
      "    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
      "    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n",
      "])\n",
      "\n",
      "# List of standard content blocks\n",
      "human_message = HumanMessage(content_blocks=[\n",
      "    {\"type\": \"text\", \"text\": \"Hello, how are you?\"},\n",
      "    {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"},\n",
      "])\n",
      "\n",
      "Specifying content_blocks when initializing a message will still populate message\n",
      "content, but provides a type-safe interface for doing so.\n",
      "â€‹Standard content blocks\n",
      "LangChain provides a standard representation for message content that works across providers.\n",
      "Message objects implement a content_blocks property that will lazily parse the content attribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking or reasoning blocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock representation:\n",
      " Anthropic OpenAICopyfrom langchain.messages import AIMessage\n",
      "\n",
      "message = AIMessage(\n",
      "    content=[\n",
      "        {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"},\n",
      "        {\"type\": \"text\", \"text\": \"...\"},\n",
      "    ],\n",
      "    response_metadata={\"model_provider\": \"anthropic\"}\n",
      ")\n",
      "message.content_blocks\n",
      "Copy[{'type': 'reasoning',\n",
      "  'reasoning': '...',\n",
      "  'extras': {'signature': 'WaUjzkyp...'}},\n",
      " {'type': 'text', 'text': '...'}]\n",
      "\n",
      "See the integrations guides to get started with the\n",
      "inference provider of your choice.\n",
      "Serializing standard contentIf an application outside of LangChain needs access to the standard content block\n",
      "representation, you can opt-in to storing content blocks in message content.To do this, you can set the LC_OUTPUT_VERSION environment variable to v1. Or,\n",
      "initialize any chat model with output_version=\"v1\":Copyfrom langchain.chat_models import init_chat_model\n",
      "\n",
      "model = init_chat_model(\"gpt-5-nano\", output_version=\"v1\")\n",
      "\n",
      "â€‹Multimodal\n",
      "Multimodality refers to the ability to work with data that comes in different\n",
      "forms, such as text, audio, images, and video. LangChain includes standard types\n",
      "for these data that can be used across providers.\n",
      "Chat models can accept multimodal data as input and generate\n",
      "it as output. Below we show short examples of input messages featuring multimodal data.\n",
      "Extra keys can be included top-level in the content block or nested in \"extras\": {\"key\": value}.OpenAI and AWS Bedrock Converse,\n",
      "for example, require a filename for PDFs. See the provider page\n",
      "for your chosen model for specifics.\n",
      "Image inputPDF document inputAudio inputVideo inputCopy# From URL\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"},\n",
      "    ]\n",
      "}\n",
      "\n",
      "# From base64 data\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\n",
      "            \"type\": \"image\",\n",
      "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
      "            \"mime_type\": \"image/jpeg\",\n",
      "        },\n",
      "    ]\n",
      "}\n",
      "\n",
      "# From provider-managed File ID\n",
      "message = {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
      "        {\"type\": \"image\", \"file_id\": \"file-abc123\"},\n",
      "    ]\n",
      "}\n",
      "\n",
      "Not all models support all file types. Check the model providerâ€™s reference for supported formats and size limits.\n",
      "â€‹Content block reference\n",
      "Content blocks are represented (either when creating a message or accessing the content_blocks property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\n",
      "CoreTextContentBlockPurpose: Standard text outputâ€‹typestringrequiredAlways \"text\"â€‹textstringrequiredThe text contentâ€‹annotationsobject[]List of annotations for the textâ€‹extrasobjectAdditional provider-specific dataExample:Copy{\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Hello world\",\n",
      "    \"annotations\": []\n",
      "}\n",
      "ReasoningContentBlockPurpose: Model reasoning stepsâ€‹typestringrequiredAlways \"reasoning\"â€‹reasoningstringThe reasoning contentâ€‹extrasobjectAdditional provider-specific dataExample:Copy{\n",
      "    \"type\": \"reasoning\",\n",
      "    \"reasoning\": \"The user is asking about...\",\n",
      "    \"extras\": {\"signature\": \"abc123\"},\n",
      "}\n",
      "MultimodalImageContentBlockPurpose: Image dataâ€‹typestringrequiredAlways \"image\"â€‹urlstringURL pointing to the image location.â€‹base64stringBase64-encoded image data.â€‹idstringReference ID to an externally stored image (e.g., in a providerâ€™s file system or in a bucket).â€‹mime_typestringImage MIME type (e.g., image/jpeg, image/png)AudioContentBlockPurpose: Audio dataâ€‹typestringrequiredAlways \"audio\"â€‹urlstringURL pointing to the audio location.â€‹base64stringBase64-encoded audio data.â€‹idstringReference ID to an externally stored audio file (e.g., in a providerâ€™s file system or in a bucket).â€‹mime_typestringAudio MIME type (e.g., audio/mpeg, audio/wav)VideoContentBlockPurpose: Video dataâ€‹typestringrequiredAlways \"video\"â€‹urlstringURL pointing to the video location.â€‹base64stringBase64-encoded video data.â€‹idstringReference ID to an externally stored video file (e.g., in a providerâ€™s file system or in a bucket).â€‹mime_typestringVideo MIME type (e.g., video/mp4, video/webm)FileContentBlockPurpose: Generic files (PDF, etc)â€‹typestringrequiredAlways \"file\"â€‹urlstringURL pointing to the file location.â€‹base64stringBase64-encoded file data.â€‹idstringReference ID to an externally stored file (e.g., in a providerâ€™s file system or in a bucket).â€‹mime_typestringFile MIME type (e.g., application/pdf)PlainTextContentBlockPurpose: Document text (.txt, .md)â€‹typestringrequiredAlways \"text-plain\"â€‹textstringThe text contentâ€‹mime_typestringMIME type of the text (e.g., text/plain, text/markdown)Tool CallingToolCallPurpose: Function callsâ€‹typestringrequiredAlways \"tool_call\"â€‹namestringrequiredName of the tool to callâ€‹argsobjectrequiredArguments to pass to the toolâ€‹idstringrequiredUnique identifier for this tool callExample:Copy{\n",
      "    \"type\": \"tool_call\",\n",
      "    \"name\": \"search\",\n",
      "    \"args\": {\"query\": \"weather\"},\n",
      "    \"id\": \"call_123\"\n",
      "}\n",
      "ToolCallChunkPurpose: Streaming tool call fragmentsâ€‹typestringrequiredAlways \"tool_call_chunk\"â€‹namestringName of the tool being calledâ€‹argsstringPartial tool arguments (may be incomplete JSON)â€‹idstringTool call identifierâ€‹indexnumber | stringPosition of this chunk in the streamInvalidToolCallPurpose: Malformed calls, intended to catch JSON parsing errors.â€‹typestringrequiredAlways \"invalid_tool_call\"â€‹namestringName of the tool that failed to be calledâ€‹argsobjectArguments to pass to the toolâ€‹errorstringDescription of what went wrongServer-Side Tool ExecutionServerToolCallPurpose: Tool call that is executed server-side.â€‹typestringrequiredAlways \"server_tool_call\"â€‹idstringrequiredAn identifier associated with the tool call.â€‹namestringrequiredThe name of the tool to be called.â€‹argsstringrequiredPartial tool arguments (may be incomplete JSON)ServerToolCallChunkPurpose: Streaming server-side tool call fragmentsâ€‹typestringrequiredAlways \"server_tool_call_chunk\"â€‹idstringAn identifier associated with the tool call.â€‹namestringName of the tool being calledâ€‹argsstringPartial tool arguments (may be incomplete JSON)â€‹indexnumber | stringPosition of this chunk in the streamServerToolResultPurpose: Search resultsâ€‹typestringrequiredAlways \"server_tool_result\"â€‹tool_call_idstringrequiredIdentifier of the corresponding server tool call.â€‹idstringIdentifier associated with the server tool result.â€‹statusstringrequiredExecution status of the server-side tool. \"success\" or \"error\".â€‹outputOutput of the executed tool.Provider-Specific BlocksNonStandardContentBlockPurpose: Provider-specific escape hatchâ€‹typestringrequiredAlways \"non_standard\"â€‹valueobjectrequiredProvider-specific data structureUsage: For experimental or provider-unique featuresAdditional provider-specific content types may be found within the reference documentation of each model provider.\n",
      "View the canonical type definitions in the API reference.\n",
      "Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.Content blocks are not a replacement for the content property, but rather a new property that can be used to access the content of a message in a standardized format.\n",
      "â€‹Use with chat models\n",
      "Chat models accept a sequence of message objects as input and return an AIMessage as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\n",
      "Refer to the below guides to learn more:\n",
      "\n",
      "Built-in features for persisting and managing conversation histories\n",
      "Strategies for managing context windows, including trimming and summarizing messages\n",
      "\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoModelsPreviousToolsNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify' metadata={'source': 'https://docs.langchain.com/oss/python/langchain/messages', 'title': 'Messages - Docs by LangChain', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(document[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577de4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
